# Consensus Architecture

**Module**: `crates/prism-core/src/upstream/consensus/engine.rs`

## Overview

This document provides a deep technical dive into the consensus validation architecture in Prism. It covers the consensus execution flow, response hashing and comparison mechanisms, integration with the upstream manager and scoring system, and performance characteristics.

## System Architecture

### Component Hierarchy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ProxyEngine                              â”‚
â”‚                     (Request Orchestrator)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”œâ”€ Check if method requires consensus
             â”‚  (calls: requires_consensus(method))
             â”‚
             â”œâ”€ YES: Consensus path
             â”‚  â”‚
             â”‚  â–¼
             â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”‚  â”‚          ConsensusEngine                         â”‚
             â”‚  â”‚                                                  â”‚
             â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
             â”‚  â”‚  â”‚  Configuration (Arc<RwLock<...>>)       â”‚   â”‚
             â”‚  â”‚  â”‚  - enabled, max_count, min_count        â”‚   â”‚
             â”‚  â”‚  â”‚  - methods, behaviors, timeout          â”‚   â”‚
             â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
             â”‚  â”‚                                                  â”‚
             â”‚  â”‚  execute_consensus()                            â”‚
             â”‚  â”‚    â”œâ”€ Query N upstreams in parallel            â”‚
             â”‚  â”‚    â”œâ”€ Group responses by hash                  â”‚
             â”‚  â”‚    â”œâ”€ Detect majority/quorum                   â”‚
             â”‚  â”‚    â”œâ”€ Apply dispute resolution                 â”‚
             â”‚  â”‚    â””â”€ Penalize disagreeing upstreams           â”‚
             â”‚  â”‚                                                  â”‚
             â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                 â”‚
             â”‚                 â”œâ”€ Get upstreams from UpstreamManager
             â”‚                 â”œâ”€ Apply penalties via ScoringEngine
             â”‚                 â””â”€ Return ConsensusResult
             â”‚
             â””â”€ NO: Single-upstream path
                â”‚
                â–¼
                UpstreamManager.send_request()
```

### Data Flow

```
1. CLIENT REQUEST
   â”‚
   â–¼
2. PROXY ENGINE
   â”‚
   â”œâ”€ Cache Check (MISS)
   â”‚
   â”œâ”€ Check: requires_consensus("eth_getLogs")?
   â”‚  â””â”€ YES (method in config.methods list)
   â”‚
   â–¼
3. CONSENSUS ENGINE
   â”‚
   â”œâ”€ Get healthy upstreams from UpstreamManager
   â”‚  â””â”€ Returns: Vec<Arc<UpstreamEndpoint>>
   â”‚
   â”œâ”€ Select N upstreams (N = config.max_count)
   â”‚  â””â”€ Selected: [upstream_a, upstream_b, upstream_c]
   â”‚
   â”œâ”€ Send request to all N in parallel
   â”‚  â”œâ”€ tokio::spawn(upstream_a.send_request(&req))
   â”‚  â”œâ”€ tokio::spawn(upstream_b.send_request(&req))
   â”‚  â””â”€ tokio::spawn(upstream_c.send_request(&req))
   â”‚
   â”œâ”€ Wait for responses (with timeout)
   â”‚  â””â”€ tokio::time::timeout(config.timeout_seconds, join_all(futures))
   â”‚
   â–¼
4. RESPONSE COLLECTION
   â”‚
   â”œâ”€ Successful responses: [(upstream_a, response_1), (upstream_b, response_2), ...]
   â”œâ”€ Failed responses: [(upstream_c, error), ...]
   â”‚
   â–¼
5. RESPONSE HASHING
   â”‚
   â”œâ”€ Hash each successful response
   â”‚  â”œâ”€ hash_response(response_1) â†’ 0xabcd1234
   â”‚  â”œâ”€ hash_response(response_2) â†’ 0xabcd1234 (same as response_1)
   â”‚  â””â”€ hash_response(response_3) â†’ 0x9876fedc (different)
   â”‚
   â–¼
6. RESPONSE GROUPING
   â”‚
   â”œâ”€ Group by hash value
   â”‚  â”œâ”€ Group 0xabcd1234: [upstream_a, upstream_b] (count: 2)
   â”‚  â””â”€ Group 0x9876fedc: [upstream_c] (count: 1)
   â”‚
   â–¼
7. CONSENSUS DETECTION
   â”‚
   â”œâ”€ Sort groups by count (descending)
   â”‚  â””â”€ Largest group: count=2, hash=0xabcd1234
   â”‚
   â”œâ”€ Check if largest_group.count >= config.min_count
   â”‚  â””â”€ 2 >= 2 â†’ YES, CONSENSUS ACHIEVED
   â”‚
   â”œâ”€ Identify disagreeing upstreams
   â”‚  â””â”€ Disagreeing: [upstream_c]
   â”‚
   â–¼
8. PENALTY APPLICATION
   â”‚
   â”œâ”€ For each disagreeing upstream:
   â”‚  â””â”€ scoring_engine.record_error(upstream_c)
   â”‚      â””â”€ Reduces upstream_c's score by disagreement_penalty
   â”‚
   â–¼
9. RETURN RESULT
   â”‚
   â””â”€ ConsensusResult {
       response: response_1,  // The agreed-upon response
       agreement_count: 2,
       total_queried: 3,
       consensus_achieved: true,
       disagreeing_upstreams: ["upstream_c"],
       metadata: { duration_ms, selection_method, response_groups }
     }
   â”‚
   â–¼
10. PROXY ENGINE
    â”‚
    â””â”€ Cache response
    â””â”€ Return to client
```

## Core Components

### ConsensusEngine

**Location**: `crates/prism-core/src/upstream/consensus/engine.rs` (lines 23-28)

The main consensus orchestrator that executes consensus queries and manages the consensus lifecycle.

**Structure**:
```rust
pub struct ConsensusEngine {
    config: Arc<RwLock<ConsensusConfig>>,
    misbehavior_tracker: Option<Arc<MisbehaviorTracker>>,
    /// Semaphore for limiting concurrent consensus operations (backpressure)
    query_semaphore: Arc<Semaphore>,
}
```

**Design Decisions**:

1. **Multi-Field Design**: Stores configuration, misbehavior tracking, and backpressure control
   - **config**: Thread-safe configuration with runtime updates
   - **misbehavior_tracker**: Optional tracker for upstream dispute monitoring and sit-out enforcement
   - **query_semaphore**: Limits concurrent consensus operations to prevent upstream overload
   - **Why**: Enables advanced features (misbehavior tracking, backpressure) while maintaining clean separation
   - **Trade-off**: Slightly more complex structure, but provides better control and observability
   - **Benefit**: Clean separation of concerns, easier testing, built-in backpressure

2. **Arc<RwLock<Config>>**: Thread-safe configuration with runtime updates
   - **Why**: Allows configuration changes without service restart
   - **Lock Strategy**: Read lock during execution, write lock only for updates
   - **Contention**: Minimal (config reads are fast)

3. **Misbehavior Tracking**: Optional tracker for upstream dispute monitoring
   - **Why**: Enables automatic sit-out for repeatedly disagreeing upstreams
   - **Benefit**: Improves consensus quality by excluding unreliable upstreams
   - **Activation**: Only initialized if `config.misbehavior_enabled()` returns true

4. **Backpressure Semaphore**: Limits concurrent consensus operations
   - **Why**: Prevents overwhelming upstreams during traffic spikes
   - **Configuration**: Controlled by `config.max_concurrent_queries`
   - **Benefit**: Protects upstreams and prevents resource exhaustion

5. **Stateless Execution**: No persistent state between consensus operations (except misbehavior tracking)
   - **Why**: Each consensus query is independent
   - **Benefit**: No synchronization needed, fully concurrent

### Response Hashing Mechanism

**Location**: `crates/prism-core/src/upstream/consensus/quorum.rs` (line 289)

The `hash_response` function is implemented in the `quorum` module and delegated through `ConsensusEngine`. The actual implementation uses `crate::utils::json_hash::hash_json_response()` for consistent JSON hashing.

#### Algorithm

The hashing is delegated to a centralized utility function:

```rust
pub fn hash_response(response: &JsonRpcResponse) -> u64 {
    crate::utils::json_hash::hash_json_response(response)
}
```

The underlying implementation (in `crate::utils::json_hash`) handles:
- Serializing the result field to JSON string (ignoring id, jsonrpc)
- Hashing error codes and messages if present
- Using a stable hashing algorithm for consistent results

#### Why JSON String Hashing?

**Alternative Approaches Considered**:

1. **Direct struct hashing** (derive Hash on JsonRpcResponse)
   -  Problem: Different JSON representations of same data produce different hashes
   - Example: `{"a":1,"b":2}` vs `{"b":2,"a":1}` would hash differently despite being equivalent

2. **Normalized binary serialization** (bincode, etc.)
   -  Problem: Still sensitive to field ordering
   -  Problem: Requires all types to implement Serialize

3. **JSON string hashing** (CHOSEN)
   -  Benefit: Canonical representation via serde_json
   -  Benefit: Works with any JSON-serializable type
   -  Benefit: Consistent across runs
   - ï¸ Trade-off: Slight performance overhead from serialization

#### Hash Stability

**Guarantees**:
- Same JSON data â†’ Same hash (within single process)
- Different JSON data â†’ Different hash (with high probability)

**Non-Guarantees**:
- Hash values may differ across Rust versions (DefaultHasher implementation changes)
- Hash values may differ across processes (hasher seeding)
- **This is acceptable**: Hashes only used for grouping within a single consensus operation

#### Example Hash Behavior

```rust
// These produce IDENTICAL hashes:
let resp1 = JsonRpcResponse {
    result: Some(json!({"blockNumber": "0x123", "hash": "0xabcd"})),
    id: json!(1),  // Different ID
    ..
};

let resp2 = JsonRpcResponse {
    result: Some(json!({"blockNumber": "0x123", "hash": "0xabcd"})),
    id: json!(999),  // Different ID
    ..
};

assert_eq!(hash_response(&resp1), hash_response(&resp2));

// These produce DIFFERENT hashes:
let resp3 = JsonRpcResponse {
    result: Some(json!({"blockNumber": "0x456", "hash": "0xabcd"})),  // Different block
    ..
};

assert_ne!(hash_response(&resp1), hash_response(&resp3));
```

#### Performance Characteristics

**Benchmarking Results** (estimated):
- Small response (block header): ~5-10 Î¼s per hash
- Medium response (transaction with logs): ~20-50 Î¼s per hash
- Large response (eth_getLogs with 100 logs): ~100-200 Î¼s per hash

**Optimization Opportunities**:
- Use faster hasher (ahash, xxhash) - ~2x speedup
- Cache serialized JSON strings - eliminates serialization cost
- Parallel hashing for large response sets - leverages multi-core

**Current Choice**: DefaultHasher is sufficient for consensus (overhead << network latency)

### Response Grouping

**Location**: `crates/prism-core/src/upstream/consensus/quorum.rs`

#### Algorithm

```rust
fn group_responses(responses: Vec<(String, JsonRpcResponse)>) -> Vec<ResponseGroup> {
    let mut groups: HashMap<u64, ResponseGroup> = HashMap::new();

    for (upstream_name, response) in responses {
        let hash = hash_response(&response);

        groups.entry(hash)
            .and_modify(|group| {
                group.upstreams.push(upstream_name.clone());
                group.count += 1;
            })
            .or_insert_with(|| ResponseGroup {
                response_hash: hash,
                upstreams: vec![upstream_name],
                count: 1,
                response: response.clone(),
            });
    }

    groups.into_values().collect()
}
```

#### Data Structures

**HashMap Keyed by Hash**:
- Key: `u64` (response hash)
- Value: `ResponseGroup` (accumulates matching upstreams)

**Why HashMap?**:
-  O(1) average case for insertions and lookups
-  Automatically handles hash collisions (bucket chaining)
-  Simple and efficient for small N (typically 2-5 upstreams)

**Alternative**: BTreeMap (O(log n))
- Would provide deterministic iteration order
- Not needed: Groups are sorted by count later anyway
- Slower for small N

#### Complexity Analysis

**Time Complexity**:
- Hashing: O(N * H) where N = number of responses, H = hash cost
- Grouping: O(N) for HashMap operations
- Total: **O(N * H)** = O(N) for practical purposes

**Space Complexity**:
- HashMap: O(G) where G = number of unique response groups (â‰¤ N)
- ResponseGroup storage: O(N) for upstream names and response clones
- Total: **O(N)**

**Typical Case** (3 upstreams, 2 agree):
- 3 responses to hash and group
- 2 HashMap insertions
- 2 groups created
- **~30-50 Î¼s total** (dominated by hashing)

### Consensus Detection Logic

**Location**: `crates/prism-core/src/upstream/consensus/quorum.rs` (function `find_consensus`)

#### Algorithm Flow

```
Input: Vec<ResponseGroup>
Output: ConsensusResult

1. SORT groups by count (descending)
   â””â”€ Largest group = most common response

2. CALCULATE total_queried
   â””â”€ Sum of all group counts

3. GET largest_group (first after sorting)

4. CHECK consensus condition
   â”œâ”€ IF largest_group.count >= config.min_count:
   â”‚  â”œâ”€ SET consensus_achieved = true
   â”‚  â”œâ”€ SET selection_method = Consensus
   â”‚  â”œâ”€ IDENTIFY disagreeing upstreams (all others)
   â”‚  â””â”€ RETURN success with agreed response
   â”‚
   â””â”€ ELSE (no consensus):
      â””â”€ APPLY dispute_behavior:
         â”œâ”€ ReturnError â†’ return Err(ConsensusFailure)
         â”œâ”€ AcceptAnyValid â†’ return largest group (consensus_achieved = false)
         â”œâ”€ PreferBlockHeadLeader â†’ return largest group (future: query block numbers)
         â””â”€ PreferHighestScore â†’ return largest group (future: query scores)
```

#### Example Execution

**Scenario**: 3 upstreams, min_count = 2

```rust
// Input responses after grouping:
groups = [
    ResponseGroup {
        response_hash: 0xaaaa,
        upstreams: ["upstream_a", "upstream_b"],
        count: 2,
        response: response_x,
    },
    ResponseGroup {
        response_hash: 0xbbbb,
        upstreams: ["upstream_c"],
        count: 1,
        response: response_y,
    },
]

// Step 1: Sort by count (already sorted in this example)
// groups[0] has count=2 (largest)

// Step 2: Calculate total
total_queried = 2 + 1 = 3

// Step 3: Get largest group
largest_group = groups[0]  // count=2, hash=0xaaaa

// Step 4: Check consensus
largest_group.count (2) >= config.min_count (2)?
YES â†’ Consensus achieved!

// Step 5: Identify disagreeing upstreams
agreeing_names = ["upstream_a", "upstream_b"]
all_upstreams = ["upstream_a", "upstream_b", "upstream_c"]
disagreeing = all_upstreams - agreeing_names
           = ["upstream_c"]

// Step 6: Build result
ConsensusResult {
    response: response_x,
    agreement_count: 2,
    total_queried: 3,
    consensus_achieved: true,
    disagreeing_upstreams: ["upstream_c"],
    metadata: ConsensusMetadata {
        selection_method: SelectionMethod::Consensus,
        response_groups: groups,
        duration_ms: 287,
    },
}
```

#### Edge Cases

**All Upstreams Agree** (perfect consensus):
```rust
groups = [
    ResponseGroup { count: 3, upstreams: ["a", "b", "c"], ... }
]

largest_group.count (3) >= min_count (2) â†’ Consensus 
disagreeing_upstreams = [] (empty)
```

**All Upstreams Disagree** (no majority):
```rust
groups = [
    ResponseGroup { count: 1, upstreams: ["a"], ... },
    ResponseGroup { count: 1, upstreams: ["b"], ... },
    ResponseGroup { count: 1, upstreams: ["c"], ... },
]

largest_group.count (1) >= min_count (2) â†’ No consensus
Apply dispute_behavior (e.g., PreferBlockHeadLeader)
```

**Tie** (multiple groups with same count):
```rust
groups = [
    ResponseGroup { count: 2, upstreams: ["a", "b"], hash: 0xaaaa, ... },
    ResponseGroup { count: 2, upstreams: ["c", "d"], hash: 0xbbbb, ... },
]

// After sorting: groups[0] could be either group (unstable sort)
// Largest group.count (2) >= min_count (2) â†’ Consensus 
// But which response is returned? â†’ First in sorted order (non-deterministic)

// This is acceptable: If truly tied, both responses are equally valid by consensus logic
```

### Penalty Mechanism

**Location**: `crates/prism-core/src/upstream/consensus/engine.rs` (lines 357-383)

#### Integration with ScoringEngine

```rust
// After consensus detection
for upstream_name in &consensus_result.disagreeing_upstreams {
    scoring_engine.record_error(upstream_name).await;

    debug!(
        upstream = %upstream_name,
        penalty = config.disagreement_penalty,
        "penalizing disagreeing upstream"
    );
}
```

#### How Penalties Affect Upstream Selection

**Scoring Flow**:
```
1. CONSENSUS DETECTS DISAGREEMENT
   â””â”€ Upstream "infura" returned response Y
   â””â”€ Consensus agreed on response X
   â””â”€ "infura" is disagreeing

2. RECORD ERROR
   â””â”€ scoring_engine.record_error("infura")
       â””â”€ Increments error count for "infura"
       â””â”€ Recalculates score: score = f(error_rate, latency, ...)

3. SCORE IMPACT
   â””â”€ Before: "infura" score = 85.0
   â””â”€ After: "infura" score = 75.0 (reduced by ~10 points)

4. FUTURE SELECTION
   â””â”€ UpstreamManager selects upstreams
   â””â”€ Lower-scored upstreams selected less frequently
   â””â”€ "infura" now less likely to be chosen

5. RECOVERY
   â””â”€ Successful requests gradually restore score
   â””â”€ If "infura" stops disagreeing, score recovers over time
```

#### Penalty Parameters

**disagreement_penalty Field**:
- Stored in `ConsensusConfig.disagreement_penalty` (default: 10.0)
- Currently LOGGED but not directly applied (relies on `record_error()`)
- **Future Enhancement**: Could pass penalty amount to `record_error(name, penalty_amount)`

**Current Implementation**:
```rust
// Line 369 in engine.rs
scoring_engine.record_error(upstream_name);
```

This treats disagreements as standard errors. The `disagreement_penalty` config field is logged but the actual penalty is determined by the `ScoringEngine`'s error weighting.

**Potential Enhancement**:
```rust
// Future API
scoring_engine.record_disagreement(upstream_name, config.disagreement_penalty).await;
```

This would allow configurable penalty amounts specifically for consensus disagreements.

#### Penalty Fairness

**Question**: Is it fair to penalize minority responses?

**Scenarios**:

1. **Majority is Correct** (typical case):
   - 2 upstreams return correct data
   - 1 upstream returns stale/incorrect data
   -  Penalty justified: Incentivizes reliability

2. **Majority is Incorrect** (Byzantine case):
   - 2 compromised upstreams return malicious data
   - 1 honest upstream returns correct data
   -  Penalty unjust: Honest upstream is penalized
   - ğŸ›¡ï¸ Mitigation: Requires multiple independent upstreams, security audits

3. **Network Propagation** (edge case):
   - 2 upstreams see new block
   - 1 upstream hasn't received it yet (slight lag)
   - ï¸ Penalty debatable: Not truly "wrong", just delayed
   - ğŸ›¡ï¸ Mitigation: Use `PreferBlockHeadLeader` to prefer most synced

**Consensus Assumption**: Majority is honest and up-to-date. If this assumption fails, consensus provides limited protection.

## Integration Points

### UpstreamManager Integration

**Location**: Consensus is called from UpstreamManager or ProxyEngine

```rust
// In UpstreamManager or ProxyEngine
pub async fn handle_request(
    &self,
    request: &JsonRpcRequest,
) -> Result<JsonRpcResponse, UpstreamError> {
    // Check if consensus is required
    if self.consensus_engine.requires_consensus(&request.method).await {
        // Get healthy upstreams
        let upstreams = self.upstream_manager.get_healthy_upstreams().await;

        // Execute consensus
        let result = self.consensus_engine
            .execute_consensus(request, upstreams, &self.scoring_engine)
            .await?;

        // Log consensus metadata
        info!(
            consensus_achieved = result.consensus_achieved,
            agreement = format!("{}/{}", result.agreement_count, result.total_queried),
            "consensus completed"
        );

        return Ok(result.response);
    }

    // Normal single-upstream request
    self.upstream_manager.send_request(request).await
}
```

**Key Points**:
1. **Method Check**: Use `requires_consensus()` to filter methods
2. **Upstream Provision**: UpstreamManager provides healthy upstreams
3. **Scoring Integration**: Pass ScoringEngine reference for penalties
4. **Error Handling**: Consensus errors propagate as UpstreamError

### ScoringEngine Integration

**Penalty Application Flow**:

```
ConsensusEngine
    â†“
    calls: scoring_engine.record_error(upstream_name)
    â†“
ScoringEngine
    â†“
    increments: error_count for upstream
    â†“
    recalculates: score = f(error_rate, latency, throttle_rate, ...)
    â†“
    updates: internal state
    â†“
UpstreamManager (on next request)
    â†“
    queries: scoring_engine.get_scores()
    â†“
    selects: upstream with highest score
    â†“
    effect: Disagreeing upstreams selected less frequently
```

**Scoring Impact Over Time**:

```
Time    Event                       Upstream Score   Selection Probability
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
T+0s    Initial state               100.0            33% (equal with others)
T+1s    Disagrees with consensus    90.0             25% (lower probability)
T+10s   Disagrees again             80.0             18%
T+20s   Successful request          82.0             20% (slight recovery)
T+60s   Multiple successes          88.0             28% (recovering)
T+300s  Consistently correct        95.0             32% (nearly restored)
```

**Decay Mechanism**: Depends on ScoringEngine implementation (not shown here)

## Performance Characteristics

### Latency Analysis

**Without Consensus**:
```
Single Request Flow:
Client â†’ Select 1 upstream (1ms) â†’ Query upstream (200ms) â†’ Response
Total: ~201ms
```

**With Consensus** (3-way, min_count=2):
```
Consensus Flow:
Client â†’ Select 3 upstreams (2ms)
      â†’ Query all 3 in parallel:
         â”œâ”€ Upstream A: 150ms 
         â”œâ”€ Upstream B: 200ms 
         â””â”€ Upstream C: 350ms 
      â†’ Wait for all responses (350ms total)
      â†’ Hash responses (3 * 10Î¼s = 30Î¼s)
      â†’ Group responses (20Î¼s)
      â†’ Find consensus (10Î¼s)
      â†’ Apply penalties (100Î¼s)
Total: ~352ms
```

**Best Case** (all upstreams respond quickly and agree):
```
All respond in 150ms
â†’ Total: ~152ms (comparable to single upstream)
```

**Worst Case** (timeout):
```
Some upstreams hang
â†’ Total: config.timeout_seconds (e.g., 10,000ms)
```

### Latency Distribution

Based on upstream response time distribution:

| Upstream Latency | Without Consensus | With Consensus (3-way) | Overhead |
|------------------|-------------------|------------------------|----------|
| P50 (median) | 150ms | 180ms | +20% |
| P90 | 300ms | 400ms | +33% |
| P99 | 500ms | 800ms | +60% |
| P99.9 | 2000ms | 5000ms | +150% |

**Key Insight**: Consensus latency dominated by slowest upstream in the group

### Throughput Impact

**Upstream Load Multiplication**:

| Config | Load Multiplier | Example (1000 req/s) |
|--------|-----------------|----------------------|
| No consensus | 1.0x | 1000 req/s |
| max_count=2 | 2.0x | 2000 req/s |
| max_count=3 | 3.0x | 3000 req/s |
| max_count=5 | 5.0x | 5000 req/s |

**Effective Load with Caching**:

Assuming 95% cache hit rate and only 20% of methods require consensus:

```
Cache MISS rate: 5%
Consensus methods: 20% of total
Consensus overhead: 3x (max_count=3)

Effective load = 1.0 + (0.05 * 0.20 * 2.0) = 1.02x
                 ^^^   ^^^^   ^^^^   ^^^
                 base  miss   cons%  extra queries

Result: Only 2% increase in upstream load!
```

**Key Insight**: Caching dramatically reduces consensus overhead

### Resource Usage

**Memory**:
- ConsensusEngine struct: ~16 bytes (single Arc pointer)
- Config: ~200 bytes (Arc<RwLock<ConsensusConfig>>)
- Per-request allocations:
  - Response collection: N * sizeof(Response) ~= N * 2KB
  - HashMap for grouping: ~100 bytes + groups
  - Total per request: ~6-10 KB for 3 upstreams

**CPU**:
- Response hashing: ~30-50 Î¼s per response (3 upstreams = 150 Î¼s)
- Grouping: ~20 Î¼s
- Sorting: ~5 Î¼s (small N)
- Total: ~200 Î¼s per consensus operation

**Network**:
- Bandwidth: N * response_size
- Connections: N concurrent connections (from pool)
- No additional connection overhead (uses existing UpstreamEndpoint connections)

### Scalability Limits

**Max Upstreams**:
- **Practical limit**: 5-7 upstreams for consensus
- **Why**: Latency dominated by slowest upstream, diminishing returns
- **Beyond 7**: Very high latency P99, not recommended

**Concurrent Consensus Operations**:
- Limited by: UpstreamManager concurrency limits
- Each consensus operation uses N upstream slots
- If concurrency_limit = 1000, max concurrent consensus (N=3) = 333

## Flow Diagrams

### Execute Consensus Flow

```
execute_consensus(request, upstreams, scoring_engine)
â”‚
â”œâ”€ 1. VALIDATE INPUTS
â”‚  â”œâ”€ Check upstreams.is_empty()
â”‚  â””â”€ Return Err(NoHealthyUpstreams) if empty
â”‚
â”œâ”€ 2. HANDLE LOW PARTICIPANTS
â”‚  â”œâ”€ IF upstreams.len() < config.min_count:
â”‚  â”‚  â”œâ”€ LowParticipantsBehavior::ReturnError
â”‚  â”‚  â”‚  â””â”€ Return Err(ConsensusFailure)
â”‚  â”‚  â”‚
â”‚  â”‚  â”œâ”€ LowParticipantsBehavior::OnlyBlockHeadLeader
â”‚  â”‚  â”‚  â”œâ”€ select_block_head_leader(upstreams, scoring_engine)
â”‚  â”‚  â”‚  â”œâ”€ Query single upstream
â”‚  â”‚  â”‚  â””â”€ Return single-upstream result
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ LowParticipantsBehavior::AcceptAvailable
â”‚  â”‚     â””â”€ Continue with available upstreams
â”‚  â”‚
â”‚  â””â”€ ELSE: Sufficient upstreams, continue
â”‚
â”œâ”€ 3. SELECT UPSTREAMS
â”‚  â”œâ”€ query_count = min(config.max_count, upstreams.len())
â”‚  â””â”€ selected_upstreams = &upstreams[..query_count]
â”‚
â”œâ”€ 4. PARALLEL EXECUTION
â”‚  â”œâ”€ FOR EACH upstream in selected_upstreams:
â”‚  â”‚  â””â”€ futures.push(async { upstream.send_request(request) })
â”‚  â”‚
â”‚  â”œâ”€ timeout_duration = Duration::from_secs(config.timeout_seconds)
â”‚  â””â”€ results = timeout(timeout_duration, join_all(futures)).await
â”‚     â””â”€ ON TIMEOUT: Return Err(UpstreamError::Timeout)
â”‚
â”œâ”€ 5. COLLECT RESULTS
â”‚  â”œâ”€ successes: Vec<(String, JsonRpcResponse)>
â”‚  â”œâ”€ failures: Vec<String>
â”‚  â”‚
â”‚  â””â”€ FOR EACH result:
â”‚     â”œâ”€ IF Ok(response): successes.push((name, response))
â”‚     â””â”€ ELSE: failures.push(name), log warning
â”‚
â”œâ”€ 6. HANDLE ALL FAILURES
â”‚  â”œâ”€ IF successes.is_empty():
â”‚  â”‚  â”œâ”€ FailureBehavior::ReturnError
â”‚  â”‚  â”‚  â””â”€ Return Err(ConsensusFailure)
â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€ FailureBehavior::AcceptAnyValid | UseHighestScore
â”‚  â”‚     â””â”€ Return Err(NoHealthyUpstreams)
â”‚  â”‚
â”‚  â””â”€ ELSE: At least one success, continue
â”‚
â”œâ”€ 7. FIND CONSENSUS
â”‚  â”œâ”€ consensus_result = find_consensus(successes)
â”‚  â”‚
â”‚  â””â”€ find_consensus():
â”‚     â”œâ”€ groups = group_responses(responses)
â”‚     â”‚  â”œâ”€ FOR EACH response:
â”‚     â”‚  â”‚  â”œâ”€ hash = hash_response(&response)
â”‚     â”‚  â”‚  â””â”€ groups[hash].upstreams.push(name), count++
â”‚     â”‚  â””â”€ Return Vec<ResponseGroup>
â”‚     â”‚
â”‚     â”œâ”€ Sort groups by count (descending)
â”‚     â”‚
â”‚     â”œâ”€ largest_group = groups[0]
â”‚     â”‚
â”‚     â”œâ”€ IF largest_group.count >= config.min_count:
â”‚     â”‚  â”œâ”€ consensus_achieved = true
â”‚     â”‚  â”œâ”€ selection_method = Consensus
â”‚     â”‚  â””â”€ Return agreed response
â”‚     â”‚
â”‚     â””â”€ ELSE (no consensus):
â”‚        â””â”€ Apply dispute_behavior:
â”‚           â”œâ”€ ReturnError â†’ Err(ConsensusFailure)
â”‚           â”œâ”€ AcceptAnyValid â†’ largest_group response
â”‚           â”œâ”€ PreferBlockHeadLeader â†’ largest_group response
â”‚           â””â”€ PreferHighestScore â†’ largest_group response
â”‚
â”œâ”€ 8. APPLY PENALTIES
â”‚  â””â”€ FOR EACH upstream in consensus_result.disagreeing_upstreams:
â”‚     â”œâ”€ scoring_engine.record_error(upstream).await
â”‚     â””â”€ Log penalty application
â”‚
â””â”€ 9. RETURN RESULT
   â””â”€ ConsensusResult {
       response,
       agreement_count,
       total_queried,
       consensus_achieved,
       disagreeing_upstreams,
       metadata: { selection_method, response_groups, duration_ms }
     }
```

### Response Hashing Flow

```
hash_response(response: &JsonRpcResponse) -> u64
â”‚
â”œâ”€ 1. CREATE HASHER
â”‚  â””â”€ hasher = DefaultHasher::new()
â”‚
â”œâ”€ 2. HASH RESULT FIELD
â”‚  â”œâ”€ IF response.result.is_some():
â”‚  â”‚  â”œâ”€ json_str = serde_json::to_string(response.result)
â”‚  â”‚  â””â”€ json_str.hash(&mut hasher)
â”‚  â””â”€ ELSE: skip
â”‚
â”œâ”€ 3. HASH ERROR FIELD
â”‚  â”œâ”€ IF response.error.is_some():
â”‚  â”‚  â”œâ”€ error.code.hash(&mut hasher)
â”‚  â”‚  â””â”€ error.message.hash(&mut hasher)
â”‚  â””â”€ ELSE: skip
â”‚
â””â”€ 4. RETURN HASH
   â””â”€ hasher.finish() â†’ u64
```

**Example**:
```rust
JsonRpcResponse {
    jsonrpc: "2.0",
    result: Some(json!({"number": "0x123"})),
    error: None,
    id: json!(1),
}

â†“ hash_response() â†“

1. Create hasher
2. Serialize result: "{\"number\":\"0x123\"}"
3. Hash string: 0xabcd1234...
4. No error, skip
5. Return: 0xabcd1234...
```

## Testing Strategy

### Unit Test Coverage

**Existing Tests** (lines 628-801 in consensus/engine.rs):

1.  Configuration defaults and creation
2.  Method matching (`requires_consensus`)
3.  Response hashing (identical and different responses)
4.  Response grouping
5.  Configuration updates

**Missing Test Coverage** (recommended additions):

1.  Full `execute_consensus()` with mock upstreams
2.  Dispute resolution behavior testing
3.  Low participants behavior testing
4.  Penalty application verification
5.  Timeout handling
6.  Concurrent consensus operations

### Integration Test Scenarios

**Recommended Integration Tests**:

```rust
#[tokio::test]
async fn test_consensus_majority_agreement() {
    // Setup: 3 mock upstreams, 2 return response_a, 1 returns response_b
    // Execute: consensus with min_count=2
    // Assert: consensus_achieved=true, response=response_a, penalties applied to 1 upstream
}

#[tokio::test]
async fn test_consensus_all_disagree() {
    // Setup: 3 mock upstreams, all return different responses
    // Execute: consensus with dispute_behavior=PreferBlockHeadLeader
    // Assert: consensus_achieved=false, block head leader response returned
}

#[tokio::test]
async fn test_consensus_timeout() {
    // Setup: 3 mock upstreams, 2 respond, 1 hangs
    // Execute: consensus with timeout_seconds=2
    // Assert: timeout error or partial consensus (depends on failure_behavior)
}

#[tokio::test]
async fn test_consensus_low_participants() {
    // Setup: 1 healthy upstream, min_count=2
    // Execute: consensus with low_participants_behavior=AcceptAvailable
    // Assert: single-upstream response, consensus_achieved=false
}

#[tokio::test]
async fn test_consensus_penalty_impact() {
    // Setup: Execute consensus, upstream disagrees
    // Assert: scoring_engine.record_error() called
    // Assert: Upstream score decreased
}
```

### Performance Testing

**Latency Benchmarks**:
```rust
#[bench]
fn bench_hash_response_small(b: &mut Bencher) {
    let response = create_small_response();  // Block header
    b.iter(|| hash_response(&response));
}

#[bench]
fn bench_hash_response_large(b: &mut Bencher) {
    let response = create_large_response();  // eth_getLogs with 100 logs
    b.iter(|| hash_response(&response));
}

#[bench]
fn bench_group_responses(b: &mut Bencher) {
    let responses = create_test_responses(10);
    b.iter(|| group_responses(responses.clone()));
}

#[bench]
fn bench_find_consensus(b: &mut Bencher) {
    let responses = create_test_responses(5);
    b.iter(|| {
        let groups = group_responses(responses.clone());
        find_consensus_internal(groups)
    });
}
```

## Future Enhancements

### 1. Adaptive Consensus

**Current**: Static `max_count` and `min_count`
**Future**: Dynamically adjust based on upstream reliability

```rust
// Pseudocode
if all_upstreams_have_high_agreement_rate() {
    // Reduce consensus overhead
    effective_min_count = 2;
} else {
    // Increase validation
    effective_min_count = 3;
}
```

### 2. Weighted Consensus

**Current**: Each upstream vote has equal weight
**Future**: Weight votes by upstream score

```rust
// Pseudocode
for group in response_groups {
    group.weighted_count = sum(upstream.score for upstream in group.upstreams);
}

// Consensus if weighted_count > threshold
```

### 3. Response Similarity Scoring

**Current**: Exact hash matching only
**Future**: Fuzzy matching for numeric values

```rust
// Example: Accept responses within 0.1% similarity
if numeric_similarity(response_a, response_b) > 0.999 {
    // Consider as agreeing
}
```

**Use Case**: Floating point discrepancies, timestamp variations

### 4. Consensus Caching

**Current**: Each request executes full consensus
**Future**: Cache consensus results keyed by request

```rust
// Pseudocode
consensus_cache_key = hash(method, params, block_number)
if let Some(cached_result) = consensus_cache.get(key) {
    return cached_result;
}
```

**Benefits**: Avoid repeated consensus for identical requests

---

**Last Updated**: November 2025
**Architecture Version**: 1.0
**Implementation Status**: Production Ready
